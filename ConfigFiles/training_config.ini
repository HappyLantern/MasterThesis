# Configuration file for model training
# Enter in the following fashion

# model-parameters
# model = 'vgg16' | 'densenet' | ...
# classes = ['1', '2', ..., '10']
# train_data, test_data paths. Validation data is taken from train_data
# 


# optimizer-hyperparameters
# ----------------------------------------------------------
# optimizer = RMSProp | AdamOptimizer | SGD
# learning_rate = float64 
# epochs = int
# early_stopping = True | False
# monitor = 'val_loss' | 'train_loss' | ?
# patience = int
# mode = 'min' | 'max' | ?
# ---------------------------------------------------------



# The single most important hyperparameter and one should always make sure that has been tuned â€” 
# Yoshua Bengio"
# 
# Perhaps add more hyperparamaters for the optimizers

[DEFAULT]
classes = ["commercial", "industrial", "residential", "parque", "parking", "forest"]
train_data = /home/kevinjohansson1995/bigger_small_subset
test_data = /home/kevinjohansson1995/bigger_small_subset_test
use_model_checkpoint = true
use_csv_logger       = true
use_reduce_lr_loss   = false
use_early_stopping   = true
use_load_weights     = true

[model-parameters]
model = vgg16bn
input_shape = [128, 128]
n_channels = 4
val_range  = 0.2

[optimizer-hyperparameters]
optimizer = RMSProp
loss = categorical_crossentropy
learning_rate = 1e-4
epochs = 50
batch_size = 64
steps_per_epoch = 25
metrics = ["acc"]

# Can add a lot more here later
[model-hyperparameters]
first_layer = 64

[model-checkpoint]
file_path = saved_models/weights.{epoch:02d}-{val_loss:.2f}.hdf5
mode = min
monitor = val_loss
save_best_only = true

[early-stopping] # if early_stopping = True
mode = min
monitor = val_loss
patience = 20

[reduce-lr-loss]
mode = min
monitor = val_loss
factor = 0.1
patience = 7
epsilon = 1e-4

[load-weights]
weights_folder = /home/kevinjohansson1995/saved_models
weights        = use-this-now.44-0.89.hdf5



