# Configuration file for model training
# Enter in the following fashion

# model-parameters
# model = 'vgg16' | 'densenet' | ...
# classes = ['1', '2', ..., '10']
# train_data, test_data paths. Validation data is taken from train_data
# 


# optimizer-hyperparameters
# ----------------------------------------------------------
# optimizer = RMSProp | AdamOptimizer | SGD
# learning_rate = float64 
# epochs = int
# early_stopping = True | False
# monitor = 'val_loss' | 'train_loss' | ?
# patience = int
# mode = 'min' | 'max' | ?
# ---------------------------------------------------------
# The single most important hyperparameter and one should always make sure that has been tuned â€” 
# Yoshua Bengio"
# 
# Perhaps add more hyperparamaters for the optimizers

[DEFAULT]
classes = ["commercial", "industrial", "residential", "parque", "parking", "forest"]
train_data = /home/kevinjohansson1995/final_data
test_data = /home/kevinjohansson1995/test_data
use_early_stopping = True
use_model_checkpoint = True
use_csv_logger = True
use_reduce_lr_loss = True

[model-parameters]
model = vgg16
input_shape = [128, 128]
n_channels = 5

[optimizer-hyperparameters]
optimizer = RMSProp
loss = categorical_crossentropy
learning_rate = 1e-4
epochs = 5
batch_size = 4
steps_per_epoch = 2
metrics = ["acc"]

[model-checkpoint]
file_path = weights.{epoch:02d}-{val_loss:.2f}.hdf5
mode = min
monitor = val_loss
save_best_only = True


[early-stopping] # if early_stopping = True
mode = min
monitor = val_loss
patience = 10

[reduce-lr-loss]
mode = min
monitor = val_loss
factor = 0.1
patience = 7
epsilon = 1e-4



# Can add a lot more here later
[model-hyperparameters]
first_layer = 64







